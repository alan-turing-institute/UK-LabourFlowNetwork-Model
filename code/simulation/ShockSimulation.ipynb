{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c0141c",
   "metadata": {},
   "source": [
    "# Model calibration\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides an overview of how to run simulations of the UK labour market model where a shock is introduced. \n",
    "\n",
    "Throughout the notebook, we use the acronyms SIC for \"Standard Industrial Classification\", SOC for \"Standard Occupational Classification\", and LFN for \"labour flow network\".\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5863ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import os\n",
    "import copy\n",
    "from joblib import Parallel, delayed\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae126cca",
   "metadata": {},
   "source": [
    "## Set variables related to file names/locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c7baf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "home =  os.getcwd()[:-4]\n",
    "\n",
    "# Choose the variables of interest\n",
    "regvar = \"GORWKR\" #geographical region \n",
    "sicvar = \"Inds07m\" #industry (SIC)\n",
    "socvar = \"SC10MMJ\" #occupation (SOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af4bcb",
   "metadata": {},
   "source": [
    "## Define all required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0f258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_sim_singleshock(i):\n",
    "    \n",
    "    \"\"\"Run a simulation including a shock to a single industry\n",
    "    \n",
    "    \n",
    "        Inputs:\n",
    "            i = indicidates the current realisation of the model\n",
    "            \n",
    "        Outputs:\n",
    "            tuple containing information on the unemployment rate and objective function value\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Run a simulation\n",
    "    model_output = sim.shock_run_simulation(shock_type, impacted_industries, input_data_dict_temp)\n",
    "    \n",
    "    pickle.dump(model_output, open(f'{home}data/model_output_shock{shock_type}_singleshock{impacted_industries}_set{k}_rep{i}.sav', 'wb'))\n",
    "    \n",
    "    # Unpack simulation output\n",
    "    wor_ids,wor_jobs,wor_job_node_ids,wor_ages,wor_consumption_prefs, \\\n",
    "                    wor_wages,wor_nonlabour_incomes, \\\n",
    "                    wor_unemp_spells,wor_employmentstatus, \\\n",
    "                    pos_ids,pos_node_ids,pos_status,pos_worker_ids, \\\n",
    "                    pos_reg,pos_sic,pos_soc,pos_wages, \\\n",
    "                    statoff_u_rates,statoff_u_durations,statoff_jtj_moves, \\\n",
    "                    statoff_num_vacancies,statoff_reg_transition_matrix, \\\n",
    "                    statoff_sic_transition_matrix,statoff_soc_transition_matrix, \\\n",
    "                    statoff_obj_vals,statoff_active_searches,impacted_industries_output = model_output\n",
    "                                                 \n",
    "                    \n",
    "    # Return info on unemployment rate and objective function only (plus info on which realisation it is)\n",
    "    return np.column_stack((statoff_u_rates, statoff_obj_vals, (i*np.ones(len(statoff_u_rates))), np.arange(0,len(statoff_u_rates)) )).copy()\n",
    "\n",
    "def parallel_sim_mc(i):\n",
    "    \n",
    "    \"\"\"Run a simulation including a shock to several industries\n",
    "    \n",
    "    \n",
    "        Inputs:\n",
    "            i = indicidates the current realisation of the model\n",
    "            \n",
    "        Outputs:\n",
    "            tuple containing information on the unemployment rate and objective function value\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Run a simulation\n",
    "    model_output = sim.shock_run_simulation(shock_type, impacted_industries, input_data_dict_temp)\n",
    "\n",
    "    pickle.dump(model_output, open(f'{home}data/model_output_shock{shock_type}_shock{n_shocked}_set{k}_rep{i}.sav', 'wb'))\n",
    "    \n",
    "    # Unpack simulation output\n",
    "    wor_ids,wor_jobs,wor_job_node_ids,wor_ages,wor_consumption_prefs, \\\n",
    "                    wor_wages,wor_nonlabour_incomes, \\\n",
    "                    wor_unemp_spells,wor_employmentstatus, \\\n",
    "                    pos_ids,pos_node_ids,pos_status,pos_worker_ids, \\\n",
    "                    pos_reg,pos_sic,pos_soc,pos_wages, \\\n",
    "                    statoff_u_rates,statoff_u_durations,statoff_jtj_moves, \\\n",
    "                    statoff_num_vacancies,statoff_reg_transition_matrix, \\\n",
    "                    statoff_sic_transition_matrix,statoff_soc_transition_matrix, \\\n",
    "                    statoff_obj_vals,statoff_active_searches,impacted_industries_output = model_output               \n",
    "             \n",
    "    # Return info on unemployment rate and objective function only (plus info on which realisation it is)\n",
    "    return np.column_stack((statoff_u_rates, statoff_obj_vals, (i*np.ones(len(statoff_u_rates))), np.arange(0,len(statoff_u_rates)) )).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0cedfb",
   "metadata": {},
   "source": [
    "## Read in all input files, and set all necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e31873",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Populate data dictionary for input into simulation function\n",
    "\n",
    "file = open(f'{home}data/activation_dict.txt', \"r\")\n",
    "contents = file.read()\n",
    "adict = ast.literal_eval(contents)\n",
    "file.close()\n",
    "\n",
    "file = open(f'{home}data/income_dict_LFS_{regvar}_{sicvar}_{socvar}.txt', \"r\")\n",
    "contents = file.read()\n",
    "idict = ast.literal_eval(contents)\n",
    "file.close()\n",
    "   \n",
    "# Global parameters\n",
    "N = 3500 # number of workers in the model\n",
    "# Vacancy rate for initializing the number of positions\n",
    "num_vac = 800000\n",
    "num_jobs = 36000000\n",
    "vacancy_rate = num_vac/num_jobs\n",
    "P = int(N*(1+vacancy_rate)) # number of positions in the model\n",
    "granularity = 1 # Granularity for timestep (1 = yearly, 12 = monthly, 52 = weekly, etc.)\n",
    "job_destruction_rate = 0.0463 # ratio of positions randomly destroyed in 1 iteration (Value from Aron's calibration)\n",
    "job_creation_rate = job_destruction_rate # ratio of positions randomly created in 1 iteration\n",
    "PD = int(job_destruction_rate*P) # Get the number of positions to be destroyed\n",
    "PC = int(job_creation_rate*P) # Get the number of positions to be created\n",
    "new_worker_init_age = 18 # Intial age of newly created workers\n",
    "## Survival rates\n",
    "# Read-in survival rate data from ONS National Life Tables (weighted mean across male and female rates, 2017-2019)\n",
    "survival_dat = pd.read_excel(f'{home}data/nationallifetable_20172019_wmeans.xlsx')\n",
    "# Convert to numpy array for computational efficiency\n",
    "worker_survival_rates = np.asarray(survival_dat.mean_survival)\n",
    "activation_rate_unemployed = adict['activation_dict']['unemployed_active_weight']/adict['activation_dict']['unemployed_weight'] #0.7923 #Rate at which unemployed workers are activated to perform a job search (set based roughly on 1 - mean economic inactivity rate for 2019, taken from https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/bulletins/employmentintheuk/april2021)\n",
    "activation_rate_employed = adict['activation_dict']['employed_active_weight']/adict['activation_dict']['employed_weight'] #Ratio controlling the relative frequency with which employed workers are actively searching (as compared to the unemployed) \n",
    "sample_size = 1 # Number of candidate positions each active worker will sample within a timestep\n",
    "gamma = 0.9662 # discount factor for utility calcs, from ONS WAS survey (pulled from Aron's code)\n",
    "#Global min/max income for generating wages\n",
    "wage_max = idict['income_dict']['max_annincome']\n",
    "wage_min = idict['income_dict']['min_annincome']\n",
    "if wage_min==0:\n",
    "    wage_min = 0.01\n",
    "    \n",
    "#Steady state convergence parameters\n",
    "ss_threshold = 0.0001 # threshold for convergence to steady state\n",
    "lag = 50 # Lag value for performing convergence calculation\n",
    "avg_length = 25 # Breadth of window to average over when performing convergence calculation\n",
    "avg_length_urates = avg_length # Breadth of window to average over when calculating steady-state unemployment rate\n",
    "t_ss=0 # Dummy value for the time for the initial flows to stabilise (leave set to 0)\n",
    "\n",
    "### Read in empirical transition matrices. convert to numpy arrays\n",
    "reg_trans_mat = pd.read_csv(open(f'{home}data/region_transitiondensity_empirical_LFS_{regvar}_{sicvar}_{socvar}.csv', 'rb'), header=0,index_col=0)\n",
    "sic_trans_mat = pd.read_csv(open(f'{home}data/sic_transitiondensities_empirical_LFS_{regvar}_{sicvar}_{socvar}.csv', 'rb'), header=0,index_col=0)\n",
    "soc_trans_mat = pd.read_csv(open(f'{home}data/soc_transitiondensities_empirical_LFS_{regvar}_{sicvar}_{socvar}.csv', 'rb'), header=0,index_col=0)\n",
    "\n",
    "reg_trans_mat = reg_trans_mat.to_numpy()\n",
    "sic_trans_mat = sic_trans_mat.to_numpy()\n",
    "soc_trans_mat = soc_trans_mat.to_numpy()\n",
    "\n",
    "### Generate category labels for region, SIC, SOC\n",
    "reg = np.arange(1,reg_trans_mat.shape[0]+1) # Regional category labels\n",
    "sic = np.arange(1,sic_trans_mat.shape[0]+1) # SIC category labels\n",
    "soc = np.arange(1,soc_trans_mat.shape[0]+1) # SOC category labels\n",
    "\n",
    "# Create list of arrays containing all possible values of the integers associated with the regions, SIC sections, and 1-digit SOC codes\n",
    "iterables = [reg, sic, soc]\n",
    "\n",
    "# Generate all possible combinations of these (region, SIC, SOC) integers (each corresponding to a potential node)\n",
    "combos = list(itertools.product(*iterables))\n",
    "\n",
    "# Create dictionary of (region, SIC, SOC) IDs for these nodes, with associated integer index values\n",
    "node_dict = {}\n",
    "for i in range(0,len(combos)):\n",
    "    node_dict[i] = combos[i] #Key is the numeric index, value is the (reg, sic, soc) triplet\n",
    "    \n",
    "n = len(node_dict) #Total number of nodes\n",
    "\n",
    "# Read in base similarity matrices\n",
    "node_reg_sim_mat = pickle.load(open(f'{home}data/reg_expanded_similaritymat_LFS.sav', 'rb')) # Region (geographical) similarity\n",
    "node_sic_sim_mat = pickle.load(open(f'{home}data/sic_expanded_similaritymat_LFS.sav', 'rb')) # SIC (industry) similarity\n",
    "node_soc_sim_mat = pickle.load(open(f'{home}data/soc_expanded_similaritymat_LFS.sav', 'rb')) # SOC (occupation) similarity\n",
    "\n",
    "# Get the number of distinct categories for each of region, sic, soc\n",
    "num_reg = len(reg)\n",
    "num_sic = len(sic)\n",
    "num_soc = len(soc)\n",
    "\n",
    "### Read in simplfied (region, sic division, 1-digit soc) distribution\n",
    "pos_dist = pd.read_csv(open(f'{home}data/positiondist_reweighted_LFS_{regvar}_{sicvar}_{socvar}.csv'))\n",
    "# pos_dist = pos_dist[pos_dist.reg_id!=22].copy()\n",
    "\n",
    "### Read in income data for generating wages\n",
    "inc_dist = pd.read_csv(open(f'{home}data/incomedist_LFS_{regvar}_{sicvar}_{socvar}.csv'))\n",
    "# inc_dist = inc_dist[inc_dist.reg_id!=22].copy()\n",
    "\n",
    "### Read in age distribution\n",
    "age_dist = pd.read_csv(open(f'{home}data/age_dist_reweighted_LFS_{regvar}_{sicvar}_{socvar}.csv'), dtype=\"float64\")['AGE']\n",
    "\n",
    "### Read in consumption preference distribution\n",
    "cpr_dist = pd.read_csv(open(f'{home}data/consumptionpref_dist_reweighted_LFS_{regvar}_{sicvar}_{socvar}.csv'))['consumption_pref']\n",
    "# cpr_dist = cpr_dist[cpr_dist>0]\n",
    "\n",
    "### Populate data dictionary for input into simulation function\n",
    "with open('%sdata/build_dict.txt' % home, 'r') as file:\n",
    "    data = file.read()    \n",
    "exec(data)\n",
    "\n",
    "# Create temporary copy of the input data dictionary to be modified during the simulations\n",
    "input_data_dict_temp = copy.deepcopy(input_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4df009",
   "metadata": {},
   "source": [
    "## Set up simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b48f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import simulation functions\n",
    "import ABMrun as sim\n",
    "\n",
    "# Set gradient descent calibration parameters (leave as-is)\n",
    "fitrun_num = 10 # Number of times to run calibration algorithm\n",
    "sim_num = 15 #number of simulations to run in parallel\n",
    "\n",
    "# Generate nu-modified similarity matrices\n",
    "reg_nu_mat = np.mean(pickle.load(open(f'{home}data/graddescent_N{N}_reps{sim_num}_GDruns{fitrun_num}_ssthresh{ss_threshold}_nus_reg_scost_mat_LFS.sav', 'rb')), axis=2)\n",
    "sic_nu_mat = np.mean(pickle.load(open(f'{home}data/graddescent_N{N}_reps{sim_num}_GDruns{fitrun_num}_ssthresh{ss_threshold}_nus_sic_scost_mat_LFS.sav', 'rb')), axis=2)\n",
    "soc_nu_mat = np.mean(pickle.load(open(f'{home}data/graddescent_N{N}_reps{sim_num}_GDruns{fitrun_num}_ssthresh{ss_threshold}_nus_soc_scost_mat_LFS.sav', 'rb')), axis=2)\n",
    "\n",
    "node_reg_sim_mat_input = np.zeros(node_reg_sim_mat.shape)\n",
    "node_sic_sim_mat_input = np.zeros(node_sic_sim_mat.shape)\n",
    "node_soc_sim_mat_input = np.zeros(node_soc_sim_mat.shape)\n",
    "\n",
    "# Modify similarity matrices using nu values\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "        node_reg_sim_mat_input[i,j] = node_reg_sim_mat[i,j]**reg_nu_mat[node_dict[i][0]-1,node_dict[j][0]-1]\n",
    "        node_sic_sim_mat_input[i,j] = node_sic_sim_mat[i,j]**sic_nu_mat[node_dict[i][1]-1,node_dict[j][1]-1]\n",
    "        node_soc_sim_mat_input[i,j] = node_soc_sim_mat[i,j]**soc_nu_mat[node_dict[i][2]-1,node_dict[j][2]-1]\n",
    "\n",
    "\n",
    "input_data_dict_temp['node_reg_sim_mat'] = node_reg_sim_mat_input\n",
    "input_data_dict_temp['node_sic_sim_mat'] = node_sic_sim_mat_input\n",
    "input_data_dict_temp['node_soc_sim_mat'] = node_soc_sim_mat_input\n",
    "\n",
    "# Proscribe shock type: position (\"position\"), wage increase (\"wageincr\"), wage decrease (\"wagedecr)\n",
    "shock_type = \"wageincr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc8a3e",
   "metadata": {},
   "source": [
    "## Run single-industry shocks\n",
    "\n",
    "Here, we systematically apply the same type of shock to each industry one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ca0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all parameters\n",
    "n_sets = 10 # Choose how many sets of simulations to run for each shocked industry\n",
    "n_mc = 15 # Choose how many simulations to run within each set (i.e. you will run n_mc simulations with the same shocked industry, and do this n_sets times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313724cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a set of Monte Carlo simulations, applying the same type of shock to each industry\n",
    "for j in np.arange(1,num_sic+1):\n",
    "    \n",
    "    impacted_industries = np.array([j]) #np.arange(1,22) \n",
    "    \n",
    "    for k in np.arange(0,n_sets):\n",
    "    \n",
    "        # Run simulations\n",
    "        result = Parallel(n_jobs=15)(delayed(parallel_sim_singleshock)(i) for i in range(n_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bbfff2",
   "metadata": {},
   "source": [
    "## Run multi-industry shocks\n",
    "\n",
    "Here, we apply a shock that impacts multiple industries at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fd960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all parameters\n",
    "n_shocked = 10 # Choose number of industries to shock\n",
    "n_sets = 25 # Choose how many sets of n_shocked industries to consider to perform simulations for\n",
    "n_mc = 15 # Choose how many simulations to run within each set (i.e. you will run n_mc simulations with the same set of n_shocked industries experiencing a shock, and do this n_sets times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92aff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a set of Monte Carlo simulations, applying the same type of shock to each set of industries\n",
    "for k in np.arange(0,n_sets):\n",
    "    \n",
    "    impacted_industries = np.random.choice(np.arange(1,num_sic+1),n_shocked,replace=False) # NB: can also manually choose a set of industries to shock\n",
    "    \n",
    "    # Run simulations\n",
    "    result = Parallel(n_jobs=15)(delayed(parallel_sim_mc)(i) for i in range(n_mc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
