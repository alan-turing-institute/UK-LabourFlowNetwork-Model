{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b214160",
   "metadata": {},
   "source": [
    "# Basic model run\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides an overview of how to run a basic simulation of the UK labour market model (i.e. without introducing shocks). \n",
    "\n",
    "Throughout the notebook, we use the acronyms SIC for \"Standard Industrial Classification\", and SOC for \"Standard Occupational Classification\".\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5863ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2b488",
   "metadata": {},
   "source": [
    "## Set variables related to file names/locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7fa94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "home =  os.getcwd()[:-4]\n",
    "\n",
    "# Choose the variables of interest\n",
    "regvar = \"GORWKR\" #geographical region \n",
    "sicvar = \"Inds07m\" #industry (SIC)\n",
    "socvar = \"SC10MMJ\" #occupation (SOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bc544",
   "metadata": {},
   "source": [
    "## Define all required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb35b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_transitions(matrix):\n",
    "    \n",
    "    \"\"\"Normalize transition matrix\n",
    "    \n",
    "        Inputs:\n",
    "            matrix = the matrix the user wishes to normalize\n",
    "\n",
    "        Outputs:\n",
    "            matrix_temp = matrix containing normalized values\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix_temp = matrix.copy()   \n",
    "    matrix_temp = matrix_temp/matrix_temp.sum().sum() #Normalize by dividing all cells by largest cell\n",
    "    \n",
    "    return matrix_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e642f",
   "metadata": {},
   "source": [
    "## Read in all input files, and set all necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e7c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Populate data dictionary for input into simulation function\n",
    "file = open(f'{home}data/20220520 KF PrePub 2001646/activation_dict.txt', \"r\")\n",
    "contents = file.read()\n",
    "adict = ast.literal_eval(contents)\n",
    "file.close()\n",
    "\n",
    "file = open(f'{home}data/20220520 KF PrePub 2001646/income_dict_LFS_{regvar}_{sicvar}_{socvar}.txt', \"r\")\n",
    "contents = file.read()\n",
    "idict = ast.literal_eval(contents)\n",
    "file.close()\n",
    "   \n",
    "# Global parameters\n",
    "N = 3500 # number of workers in the model\n",
    "# Vacancy rate for initializing the number of positions\n",
    "num_vac = 800000\n",
    "num_jobs = 36000000\n",
    "vacancy_rate = num_vac/num_jobs\n",
    "P = int(N*(1+vacancy_rate)) # number of positions in the model\n",
    "granularity = 1 # Granularity for timestep (1 = yearly, 12 = monthly, 52 = weekly, etc.)\n",
    "job_destruction_rate = 0.0463 # ratio of positions randomly destroyed in 1 iteration (Value from Aron's calibration)\n",
    "job_creation_rate = job_destruction_rate # ratio of positions randomly created in 1 iteration\n",
    "PD = int(job_destruction_rate*P) # Get the number of positions to be destroyed\n",
    "PC = int(job_creation_rate*P) # Get the number of positions to be created\n",
    "new_worker_init_age = 18 # Intial age of newly created workers\n",
    "## Survival rates\n",
    "# Read-in survival rate data from ONS National Life Tables (weighted mean across male and female rates, 2017-2019)\n",
    "survival_dat = pd.read_excel(f'{home}data/nationallifetable_20172019_wmeans.xlsx')\n",
    "# Convert to numpy array for computational efficiency\n",
    "worker_survival_rates = np.asarray(survival_dat.mean_survival)\n",
    "activation_rate_unemployed = adict['activation_dict']['unemployed_active_weight']/adict['activation_dict']['unemployed_weight'] #0.7923 #Rate at which unemployed workers are activated to perform a job search (set based roughly on 1 - mean economic inactivity rate for 2019, taken from https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/bulletins/employmentintheuk/april2021)\n",
    "activation_rate_employed = adict['activation_dict']['employed_active_weight']/adict['activation_dict']['employed_weight'] #Ratio controlling the relative frequency with which employed workers are actively searching (as compared to the unemployed) \n",
    "sample_size = 1 # Number of candidate positions each active worker will sample within a timestep\n",
    "gamma = 0.9662 # discount factor for utility calculations\n",
    "#Global min/max income for generating wages\n",
    "wage_max = idict['income_dict']['max_annincome']\n",
    "wage_min = idict['income_dict']['min_annincome']\n",
    "if wage_min==0:\n",
    "    wage_min = 0.01\n",
    "    \n",
    "#Steady state convergence parameters\n",
    "ss_threshold = 0.0001 # threshold for convergence to steady state\n",
    "lag = 50 # Lag value for performing convergence calculation\n",
    "avg_length = 25 # Breadth of window to average over when performing convergence calculation\n",
    "avg_length_urates = avg_length # Breadth of window to average over when calculating steady-state unemployment rate\n",
    "t_ss=0 # Dummy value for the time for the initial flows to stabilise (leave set to 0)\n",
    "\n",
    "### Read in empirical transition matrices. convert to numpy arrays\n",
    "reg_trans_mat = pd.read_csv(open(f'{home}data/20220520 KF PrePub 2001646/region_transitiondensity_empirical_LFS_{regvar}_{sicvar}_{socvar}.csv', 'rb'), header=0,index_col=0)\n",
    "sic_trans_mat = pd.read_csv(open(f'{home}data/20220520 KF PrePub 2001646/sic_transitiondensities_empirical_LFS_{regvar}_{sicvar}_{socvar}.csv', 'rb'), header=0,index_col=0)\n",
    "soc_trans_mat = pd.read_csv(open(f'{home}data/20220520 KF PrePub 2001646/soc_transitiondensities_empirical_LFS_{regvar}_{sicvar}_{socvar}.csv', 'rb'), header=0,index_col=0)\n",
    "\n",
    "reg_trans_mat = reg_trans_mat.to_numpy()\n",
    "sic_trans_mat = sic_trans_mat.to_numpy()\n",
    "soc_trans_mat = soc_trans_mat.to_numpy()\n",
    "\n",
    "### Generate category labels for region, SIC, SOC\n",
    "reg = np.arange(1,reg_trans_mat.shape[0]+1) # Regional category labels\n",
    "sic = np.arange(1,sic_trans_mat.shape[0]+1) # SIC category labels\n",
    "soc = np.arange(1,soc_trans_mat.shape[0]+1) # SOC category labels\n",
    "\n",
    "# Create list of arrays containing all possible values of the integers associated with the regions, SIC sections, and 1-digit SOC codes\n",
    "iterables = [reg, sic, soc]\n",
    "\n",
    "# Generate all possible combinations of these (region, SIC, SOC) integers (each corresponding to a potential node)\n",
    "combos = list(itertools.product(*iterables))\n",
    "\n",
    "# Create dictionary of (region, SIC, SOC) IDs for these nodes, with associated integer index values\n",
    "node_dict = {}\n",
    "for i in range(0,len(combos)):\n",
    "    node_dict[i] = combos[i] #Key is the numeric index, value is the (reg, sic, soc) triplet\n",
    "    \n",
    "n = len(node_dict) #Total number of nodes\n",
    "\n",
    "# Read in base similarity matrices\n",
    "node_reg_sim_mat = pickle.load(open(f'{home}data/reg_expanded_similaritymat_LFS.sav', 'rb')) # Region (geographical) similarity\n",
    "node_sic_sim_mat = pickle.load(open(f'{home}data/sic_expanded_similaritymat_LFS.sav', 'rb')) # SIC (industry) similarity\n",
    "node_soc_sim_mat = pickle.load(open(f'{home}data/soc_expanded_similaritymat_LFS.sav', 'rb')) # SOC (occupation) similarity\n",
    "\n",
    "# Get the number of distinct categories for each of region, sic, soc\n",
    "num_reg = len(reg)\n",
    "num_sic = len(sic)\n",
    "num_soc = len(soc)\n",
    "\n",
    "### Read in simplfied (region, sic division, 1-digit soc) distribution\n",
    "pos_dist = pd.read_csv(open(f'{home}data/positiondist_reweighted_LFS_{regvar}_{sicvar}_{socvar}.csv'))\n",
    "\n",
    "### Read in income data for generating wages\n",
    "inc_dist = pd.read_csv(open(f'{home}data/20220520 KF PrePub 2001646/incomedist_LFS_{regvar}_{sicvar}_{socvar}.csv'))\n",
    "inc_dist = inc_dist[inc_dist.reg_id!=22].copy()\n",
    "\n",
    "### Read in age distribution\n",
    "age_dist = pd.read_csv(open(f'{home}data/age_dist_reweighted_LFS_{regvar}_{sicvar}_{socvar}.csv'), dtype=\"float64\")['AGE']\n",
    "\n",
    "### Read in consumption preference distribution\n",
    "cpr_dist = pd.read_csv(open(f'{home}data/consumptionpref_dist_reweighted_LFS_{regvar}_{sicvar}_{socvar}.csv'))['consumption_pref']\n",
    "# cpr_dist = cpr_dist[cpr_dist>0]\n",
    "\n",
    "### Populate data dictionary for input into simulation function\n",
    "with open('%sdata/build_dict.txt' % home, 'r') as file:\n",
    "    data = file.read()    \n",
    "exec(data)\n",
    "\n",
    "# Create temporary copy of the input data dictionary to be modified during the simulations\n",
    "input_data_dict_temp = copy.deepcopy(input_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae8fbf",
   "metadata": {},
   "source": [
    "## Set up simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf18e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import simulation functions\n",
    "import ABMrun as sim\n",
    "\n",
    "steady_state_length = 250 # Number of rounds to estimate transition matrices in the steady state of the model\n",
    "fitrun_num = 10 # Run of calibration algorithm - leave as is\n",
    "sim_num = 15 #number of simulations to run in parallel during calibratiion - leave as is\n",
    "\n",
    "# Generate nu-modified similarity matrices\n",
    "reg_nu_mat = np.mean(pickle.load(open(f'{home}data/calibration/graddescent_N{N}_reps{sim_num}_GDruns{fitrun_num}_ssthresh{ss_threshold}_nus_reg_scost_mat_LFS.sav', 'rb')), axis=2)\n",
    "sic_nu_mat = np.mean(pickle.load(open(f'{home}data/calibration/graddescent_N{N}_reps{sim_num}_GDruns{fitrun_num}_ssthresh{ss_threshold}_nus_sic_scost_mat_LFS.sav', 'rb')), axis=2)\n",
    "soc_nu_mat = np.mean(pickle.load(open(f'{home}data/calibration/graddescent_N{N}_reps{sim_num}_GDruns{fitrun_num}_ssthresh{ss_threshold}_nus_soc_scost_mat_LFS.sav', 'rb')), axis=2)\n",
    "\n",
    "node_reg_sim_mat_input = np.zeros(node_reg_sim_mat.shape)\n",
    "node_sic_sim_mat_input = np.zeros(node_sic_sim_mat.shape)\n",
    "node_soc_sim_mat_input = np.zeros(node_soc_sim_mat.shape)\n",
    "\n",
    "# Modify similarity matrices using nu-values\n",
    "for i in range(0,n):\n",
    "    for j in range(0,n):\n",
    "        node_reg_sim_mat_input[i,j] = node_reg_sim_mat[i,j]**reg_nu_mat[node_dict[i][0]-1,node_dict[j][0]-1]\n",
    "        node_sic_sim_mat_input[i,j] = node_sic_sim_mat[i,j]**sic_nu_mat[node_dict[i][1]-1,node_dict[j][1]-1]\n",
    "        node_soc_sim_mat_input[i,j] = node_soc_sim_mat[i,j]**soc_nu_mat[node_dict[i][2]-1,node_dict[j][2]-1]\n",
    "\n",
    "\n",
    "input_data_dict_temp['node_reg_sim_mat'] = node_reg_sim_mat_input\n",
    "input_data_dict_temp['node_sic_sim_mat'] = node_sic_sim_mat_input\n",
    "input_data_dict_temp['node_soc_sim_mat'] = node_soc_sim_mat_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b9117",
   "metadata": {},
   "source": [
    "## Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1bce8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 100 timesteps to reach the steady state.\n",
      "It took 85 additional timesteps for the new flows to stabilise.\n"
     ]
    }
   ],
   "source": [
    "# Run a simulation\n",
    "model_output = sim.extended_run_simulation(input_data_dict_temp)\n",
    "# Unpack simulation output\n",
    "wor_ids,wor_jobs,wor_job_node_ids,wor_ages,wor_consumption_prefs, \\\n",
    "                wor_wages,wor_nonlabour_incomes, \\\n",
    "                wor_unemp_spells,wor_employmentstatus, \\\n",
    "                pos_ids,pos_node_ids,pos_status,pos_worker_ids, \\\n",
    "                pos_reg,pos_sic,pos_soc,pos_wages, \\\n",
    "                statoff_u_rates,statoff_u_durations,statoff_jtj_moves, \\\n",
    "                statoff_num_vacancies,statoff_reg_transition_matrix, \\\n",
    "                statoff_sic_transition_matrix,statoff_soc_transition_matrix, \\\n",
    "                statoff_obj_vals,statoff_active_searches = model_output\n",
    "\n",
    "# Save model output\n",
    "pickle.dump(model_output, open('%sdata/model_output_singlerun.sav' % home, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814b08b",
   "metadata": {},
   "source": [
    "## Visualise output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "571edebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg_trans_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v2/cp9hnv391c54b7vm_p08q97m0000gr/T/ipykernel_23007/3620769618.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set plotting parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m maxval = max([reg_trans_mat.max().max(), sic_trans_mat.max().max(), soc_trans_mat.max().max(), \\\n\u001b[0m\u001b[1;32m      3\u001b[0m               \u001b[0mnormalized_transitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatoff_reg_transition_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mnormalized_transitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatoff_reg_transition_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               normalized_transitions(statoff_reg_transition_matrix).max().max()])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg_trans_mat' is not defined"
     ]
    }
   ],
   "source": [
    "        # Set plotting parameter\n",
    "        maxval = max([reg_trans_mat.max().max(), sic_trans_mat.max().max(), soc_trans_mat.max().max(), \\\n",
    "                      normalized_transitions(statoff_reg_transition_matrix).max().max(), \\\n",
    "                      normalized_transitions(statoff_reg_transition_matrix).max().max(), \\\n",
    "                      normalized_transitions(statoff_reg_transition_matrix).max().max()])\n",
    "    \n",
    "        # Plot of job-to-job transitions\n",
    "        plt.figure(1, figsize=(15.5,8))\n",
    "\n",
    "        plt.subplot(231)\n",
    "        g1=sns.heatmap(reg_trans_mat, linewidth=0.05, cmap=\"magma_r\", vmin=0, vmax=maxval, cbar_ax=None)\n",
    "        g1.set_ylabel(\"Previous geographical region\")\n",
    "        g1.set_title(\"Geographical transition densities (observed)\")\n",
    "\n",
    "        plt.subplot(232)\n",
    "        g2=sns.heatmap(sic_trans_mat, linewidth=0.05, cmap=\"magma_r\", vmin=0, vmax=maxval, cbar_ax=None)\n",
    "\n",
    "        g2.set_ylabel(\"Previous industry\")\n",
    "        g2.set_title(\"Industry transition densities (observed)\")\n",
    "\n",
    "        plt.subplot(233)\n",
    "        g3=sns.heatmap(soc_trans_mat, linewidth=0.05, cmap=\"magma_r\", vmin=0, vmax=maxval)\n",
    "        g3.set_ylabel(\"Previous occupation\")\n",
    "        g3.set_title(\"Occupation transition densities (observed)\")\n",
    "\n",
    "        plt.subplot(234)\n",
    "        g4=sns.heatmap(normalized_transitions(statoff_reg_transition_matrix), linewidth=0.05, cmap=\"magma_r\", vmin=0, vmax=maxval, cbar_ax=None)\n",
    "        g4.set_xlabel(\"Next geographical region\")\n",
    "        g4.set_ylabel(\"Previous geographical region\")\n",
    "        g4.set_title(\"Geographical transition densities (simulated)\")\n",
    "\n",
    "        plt.subplot(235)\n",
    "        g5=sns.heatmap(normalized_transitions(statoff_sic_transition_matrix), linewidth=0.05, cmap=\"magma_r\", vmin=0, vmax=maxval, cbar_ax=None)\n",
    "        g5.set_xlabel(\"Next industry\")\n",
    "        g5.set_ylabel(\"Previous industry\")\n",
    "        g5.set_title(\"Industry transition densities (simulated)\")\n",
    "\n",
    "        plt.subplot(236)\n",
    "        g6=sns.heatmap(normalized_transitions(statoff_soc_transition_matrix) , linewidth=0.05, cmap=\"magma_r\", vmin=0, vmax=maxval, cbar_ax=None)\n",
    "        g6.set_xlabel(\"Next occupation\")\n",
    "        g6.set_ylabel(\"Previous occupation\")\n",
    "        g6.set_title(\"Occupation transition densities (simulated)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682fc7f",
   "metadata": {},
   "source": [
    "## Calculate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330b65c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between observed and simulated LFNs is 0.9591063692177293\n",
      "Frobenius norm of differences between observed and simulated LFNs is 0.06497264532951971\n"
     ]
    }
   ],
   "source": [
    "#Compare empirical transition probability matrix with similarity matrix\n",
    "# Regions\n",
    "#Pearson correlation\n",
    "reg_pearson=(np.corrcoef(np.asarray(reg_trans_mat).flatten(),np.asarray(sim.normalized_transitions(statoff_reg_transition_matrix)).flatten()))\n",
    "# Frobeneius norm\n",
    "reg_frobenius=(np.linalg.norm(np.asarray(reg_trans_mat).flatten()-np.asarray(sim.normalized_transitions(statoff_reg_transition_matrix)).flatten()))\n",
    "# Industries (SIC)\n",
    "#Pearson correlation\n",
    "sic_pearson=(np.corrcoef(np.asarray(sic_trans_mat).flatten(),np.asarray(sim.normalized_transitions(statoff_sic_transition_matrix)).flatten()))\n",
    "# Frobeneius norm\n",
    "sic_frobenius=(np.linalg.norm(np.asarray(sic_trans_mat).flatten()-np.asarray(sim.normalized_transitions(statoff_sic_transition_matrix)).flatten()))\n",
    "# Occupation (SOC)\n",
    "#Pearson correlation\n",
    "soc_pearson=(np.corrcoef(np.asarray(soc_trans_mat).flatten(),np.asarray(sim.normalized_transitions(statoff_soc_transition_matrix)).flatten()))\n",
    "# Frobeneius norm\n",
    "soc_frobenius=(np.linalg.norm(np.asarray(soc_trans_mat).flatten()-np.asarray(sim.normalized_transitions(statoff_soc_transition_matrix)).flatten()))\n",
    "#Combined (equal weighting)\n",
    "#Pearson    \n",
    "comb_pearson = 1/3*(reg_pearson+sic_pearson+soc_pearson)\n",
    "#Frobenius  \n",
    "comb_frobenius = 1/3*(reg_frobenius+sic_frobenius+soc_frobenius)\n",
    "\n",
    "print(f'Pearson correlation between observed and simulated LFNs is {comb_pearson[0,1]}')\n",
    "print(f'Frobenius norm of differences between observed and simulated LFNs is {comb_frobenius}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
